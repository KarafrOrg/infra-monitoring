object-buckets:
  buckets:
    - name: &tempoTraceBucketName tempo-traces-bucket
      storageClassName: rook-ceph-bucket
      labels:
        app: loki
      pushSecret:
        enabled: true
        secretStoreName: gcp-sm-cluster-secret-store
        secretMappings:
          - secretKey: AWS_ACCESS_KEY_ID
            remoteKey: tempo-traces-bucket-s3-access-key
          - secretKey: AWS_SECRET_ACCESS_KEY
            remoteKey: tempo-traces-bucket-s3-secret-key

tempo:
  replicas: 3

  tempo:
    metricsGenerator:
      enabled: true
      remoteWriteUrl: "http://mimir-gateway.infra-monitoring.svc.cluster.local:80/api/v1/push"
      processor:
        service_graphs:
          dimensions: [ ]
        span_metrics:
          dimensions: [ ]
        local_blocks:
          filter_server_spans: false
    extraEnv:
      - name: AWS_ACCESS_KEY_ID
        valueFrom:
          secretKeyRef:
            name: *tempoTraceBucketName
            key: AWS_ACCESS_KEY_ID
      - name: AWS_SECRET_ACCESS_KEY
        valueFrom:
          secretKeyRef:
            name: *tempoTraceBucketName
            key: AWS_SECRET_ACCESS_KEY
      - name: TEMPO_S3_BUCKET_NAME
        valueFrom:
          configMapKeyRef:
            name: *tempoTraceBucketName
            key: BUCKET_NAME
      - name: TEMPO_S3_ENDPOINT
        valueFrom:
          configMapKeyRef:
            name: *tempoTraceBucketName
            key: BUCKET_HOST
      - name: TEMPO_S3_PORT
        valueFrom:
          configMapKeyRef:
            name: *tempoTraceBucketName
            key: BUCKET_PORT
    extraArgs:
      config.expand-env: true

    # Server configuration for query frontend
    server:
      http_listen_port: 3100
      grpc_listen_port: 9095

    # Enable multitenancy (false for single tenant, consistent queries)
    multitenancy_enabled: false

    storage:
      trace:
        backend: s3
        s3:
          bucket: ${TEMPO_S3_BUCKET_NAME}
          endpoint: ${TEMPO_S3_ENDPOINT}.cluster.local:${TEMPO_S3_PORT}
          region: us-east-1
          insecure: true
    # Retention and compaction
    compactor:
      compaction:
        block_retention: 168h  # 7 days

    # Ingester configuration
    ingester:
      max_block_duration: 10m
      complete_block_timeout: 15m
      flush_check_period: 5s
      trace_idle_period: 10s

    # Receivers configuration
    receivers:
      otlp:
        protocols:
          http:
            endpoint: 0.0.0.0:4318
          grpc:
            endpoint: 0.0.0.0:4317
      jaeger:
        protocols:
          grpc:
            endpoint: 0.0.0.0:14250
          thrift_http:
            endpoint: 0.0.0.0:14268
          thrift_compact:
            endpoint: 0.0.0.0:6831
          thrift_binary:
            endpoint: 0.0.0.0:6832
      zipkin:
        endpoint: 0.0.0.0:9411

    querier:
      max_concurrent_queries: 20
      frontend_worker:
        frontend_address: localhost:9095
        parallelism: 2

    queryFrontend:
      search:
        max_duration: 0  # unlimited search duration
        max_result_limit: 1000
      trace_by_id:
        query_shards: 2

    overrides:
      defaults:
        metrics_generator:
          processors: [ service-graphs, span-metrics, local-blocks ]

  resources:
    requests:
      cpu: 100m
      memory: 512Mi
    limits:
      cpu: 500m
      memory: 2Gi

  # Persistent storage for WAL
  persistence:
    enabled: true
    size: 30Gi
    storageClassName: longhorn

  # Service configuration
  service:
    type: ClusterIP
    port: 3100

  # Enable OTLP receivers
  traces:
    otlp:
      http:
        enabled: true
      grpc:
        enabled: true
    jaeger:
      thriftHttp:
        enabled: true
      grpc:
        enabled: true
    zipkin:
      enabled: true

  # Service monitor for Prometheus
  serviceMonitor:
    enabled: true
    interval: 30s

  # Pod annotations
  podAnnotations:
    prometheus.io/scrape: "true"
    prometheus.io/port: "3100"

  # Security context
  securityContext:
    runAsNonRoot: true
    runAsUser: 10001
    fsGroup: 10001

  # Topology spread for high availability (even in single replica mode)
  topologySpreadConstraints:
    - maxSkew: 1
      topologyKey: kubernetes.io/hostname
      whenUnsatisfiable: ScheduleAnyway
      labelSelector:
        matchLabels:
          app.kubernetes.io/name: tempo
          app.kubernetes.io/instance: tempo
